{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMnhdsaQSupwUKLY4JzJMuf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enggarpramoto23-a11y/Deep-Learning-Lanjut/blob/main/Pak_Anam_Pert11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2su6VVyZI6ba"
      },
      "outputs": [],
      "source": [
        "!pip install datasets diffusers transformers accelerate\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "# 1. Load Dataset\n",
        "print(\"Memuatdataset...\")\n",
        "raw_dataset= load_dataset(\"reach-vb/pokemon-blip-captions\", split=\"train\")\n",
        "# 2. Ambil daftar caption untukproses adaptasi teks\n",
        "all_captions= [item['text'] for item in raw_dataset]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Setup Text Vectorization\n",
        "max_tokens = 5000\n",
        "seq_len = 20\n",
        "\n",
        "text_vectorizer = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_sequence_length=seq_len,\n",
        ")\n",
        "\n",
        "# Proses Adapt (mempelajari kosakata dari dataset)\n",
        "text_vectorizer.adapt(all_captions)\n",
        "\n",
        "vocab = text_vectorizer.get_vocabulary()\n",
        "\n",
        "print(f\"Kamus Teks Berhasil Dibuat. Jumlah kosakata: {len(vocab)}\")\n",
        "print(\"Contoh 10 kata pertama:\", vocab[:10])\n"
      ],
      "metadata": {
        "id": "K7aqU8juKh0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_fn(item):\n",
        "    # Proses Gambar\n",
        "    image = item['image'].convert(\"RGB\").resize((64, 64))\n",
        "    image = np.array(image) / 255.0  # Normalisasi0-1\n",
        "    # Proses Teks\n",
        "    caption = item['text']\n",
        "    return caption, image\n",
        "# Membuatgenerator dataset\n",
        "def gen():\n",
        "    for item in raw_dataset:\n",
        "        yield preprocess_fn(item)\n",
        "\n",
        "# Membuattf.data.Dataset\n",
        "train_ds= tf.data.Dataset.from_generator(\n",
        "    gen,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(), dtype=tf.string),\n",
        "        tf.TensorSpec(shape=(64, 64, 3), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "# Batching dan TransformasiTeks keAngka\n",
        "train_ds= train_ds.map(lambda x, y: (text_vectorizer(x), y))\n",
        "train_ds = train_ds.batch(16).shuffle(100).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "5g1AJnAjKzBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PokemonTrainer(keras.Model):\n",
        "    def __init__(self, transformer, vqvae_encoder):\n",
        "        super().__init__()\n",
        "        self.transformer= transformer\n",
        "        self.vqvae_encoder= vqvae_encoder\n",
        "        self.loss_tracker= keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "    def train_step(self, data):\n",
        "        text_tokens, images = data\n",
        "        # 1. Ubahgambaraslimenjaditoken visual menggunakanencoder\n",
        "        # Kita simulasikandenganoutput dummy sesuaiukuranlatent grid (misal16x16)\n",
        "        visual_tokens= tf.random.uniform((tf.shape(images)[0], 256), minval=0, maxval=1024, dtype=tf.int32)\n",
        "\n",
        "        # 2. Siapkaninput dan target (Autoregressive)\n",
        "        vis_input= visual_tokens[:, :-1]\n",
        "        vis_target= visual_tokens[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Prediksi\n",
        "            preds = self.transformer([text_tokens, vis_input], training=True)\n",
        "            # HitungLoss\n",
        "            loss = keras.losses.sparse_categorical_crossentropy(vis_target, preds, from_logits=True)\n",
        "        grads = tape.gradient(loss, self.transformer.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.transformer.trainable_variables))\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}"
      ],
      "metadata": {
        "id": "PutXjquCLGPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PokemonTrainer(keras.Model):\n",
        "    def __init__(self, transformer, vqvae_encoder):\n",
        "        super().__init__()\n",
        "        self.transformer = transformer\n",
        "        self.vqvae_encoder = vqvae_encoder\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        text_tokens, images = data\n",
        "\n",
        "        # Encode image → visual tokens\n",
        "        # Menggunakan output dummy untuk visual_tokens untuk saat ini\n",
        "        visual_tokens = tf.random.uniform((tf.shape(images)[0], 256), minval=0, maxval=1024, dtype=tf.int32)\n",
        "\n",
        "        # Autoregressive shift\n",
        "        vis_input = visual_tokens[:, :-1]\n",
        "        vis_target = visual_tokens[:, 1:]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = self.transformer(\n",
        "                [text_tokens, vis_input],\n",
        "                training=True\n",
        "            )\n",
        "\n",
        "            loss = keras.losses.sparse_categorical_crossentropy(\n",
        "                vis_target, preds, from_logits=True\n",
        "            )\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        grads = tape.gradient(loss, self.transformer.trainable_variables)\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(grads, self.transformer.trainable_variables)\n",
        "        )\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "# --- Definisi model yang hilang akan ditambahkan di sini ---\n",
        "# Dummy transformer_model dan vqvae_encoder untuk memungkinkan eksekusi\n",
        "# Anda perlu mengganti ini dengan implementasi model yang sebenarnya\n",
        "# Misalnya:\n",
        "num_visual_tokens = 1024 # Contoh, sesuaikan dengan ukuran vocabulary VQ-VAE Anda\n",
        "embedding_dim = 256 # Contoh, ukuran embedding\n",
        "num_heads = 4 # Contoh, jumlah attention heads\n",
        "ff_dim = 1024 # Contoh, dimensi feed-forward\n",
        "\n",
        "# Dummy VQVAE Encoder (hanya sebagai placeholder)\n",
        "vqvae_encoder = keras.Sequential([\n",
        "    layers.Input(shape=(64, 64, 3)),\n",
        "    layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
        "    layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation=\"relu\") # Output ukuran yang sama dengan visual_tokens dummy\n",
        "], name=\"vqvae_encoder_dummy\")\n",
        "\n",
        "# Dummy Transformer (membutuhkan input teks dan visual)\n",
        "text_input = keras.Input(shape=(seq_len,), dtype=\"int32\", name=\"text_input\")\n",
        "visual_input = keras.Input(shape=(255,), dtype=\"int32\", name=\"visual_input\") # 256-1 = 255\n",
        "\n",
        "text_embeddings = layers.Embedding(max_tokens, embedding_dim)(text_input)\n",
        "visual_embeddings = layers.Embedding(num_visual_tokens, embedding_dim)(visual_input)\n",
        "\n",
        "# Concatenate text and visual embeddings\n",
        "x = layers.Concatenate(axis=1)([text_embeddings, visual_embeddings])\n",
        "x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(x, x)\n",
        "x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "\n",
        "# Extract only the visual part of the output sequence to match vis_target length\n",
        "visual_output_sequence = x[:, seq_len:, :]\n",
        "\n",
        "# Remove softmax activation, as sparse_categorical_crossentropy expects logits with from_logits=True\n",
        "outputs = layers.Dense(num_visual_tokens)(visual_output_sequence)\n",
        "\n",
        "transformer_model = keras.Model(inputs=[text_input, visual_input], outputs=outputs, name=\"transformer_model_dummy\")\n",
        "\n",
        "# Inisialisasi dan compile\n",
        "trainer = PokemonTrainer(transformer_model, vqvae_encoder)\n",
        "trainer.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
        ")\n",
        "\n",
        "# Jalankan training\n",
        "print(\"Memulai pelatihan...\")\n",
        "trainer.fit(train_ds, epochs=10)  # coba 10 epoch dulu"
      ],
      "metadata": {
        "id": "SBrh8-YSLHDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# =========================================================\n",
        "# 1. AUTOREGRESSIVE VISUAL TOKEN GENERATION\n",
        "# =========================================================\n",
        "\n",
        "def generate_image_tokens(\n",
        "    transformer_model,\n",
        "    text_tokens,\n",
        "    num_visual_tokens=256\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate visual tokens autoregressively\n",
        "    Output shape: (batch, num_visual_tokens)\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = tf.shape(text_tokens)[0]\n",
        "    generated_tokens = tf.zeros((batch_size, 0), dtype=tf.int32)\n",
        "\n",
        "    for step in range(num_visual_tokens):\n",
        "        cur_len = tf.shape(generated_tokens)[1]\n",
        "\n",
        "        # Pad or crop visual input to (num_visual_tokens - 1)\n",
        "        if cur_len < num_visual_tokens - 1:\n",
        "            pad_len = (num_visual_tokens - 1) - cur_len\n",
        "            visual_input = tf.concat(\n",
        "                [generated_tokens,\n",
        "                 tf.zeros((batch_size, pad_len), tf.int32)],\n",
        "                axis=1\n",
        "            )\n",
        "        else:\n",
        "            visual_input = generated_tokens[:, -(num_visual_tokens - 1):]\n",
        "\n",
        "        # Transformer forward\n",
        "        logits = transformer_model(\n",
        "            [text_tokens, visual_input],\n",
        "            training=False\n",
        "        )\n",
        "\n",
        "        # Predict next token (always take the last prediction from the fixed-length output)\n",
        "        next_logits = logits[:, -1, :]\n",
        "        next_token = tf.argmax(next_logits, axis=-1, output_type=tf.int32)\n",
        "        next_token = tf.expand_dims(next_token, axis=1)\n",
        "\n",
        "        # Append\n",
        "        generated_tokens = tf.concat(\n",
        "            [generated_tokens, next_token],\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "    return generated_tokens\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 2. DUMMY DECODER → GAMBAR RGB BERWARNA\n",
        "# =========================================================\n",
        "\n",
        "def decode_to_real_image(visual_tokens, image_size=64):\n",
        "    \"\"\"\n",
        "    Convert visual tokens into colorful RGB image (dummy VQ-VAE)\n",
        "    Output shape: (64, 64, 3)\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = visual_tokens[0].numpy()\n",
        "\n",
        "    grid_size = int(np.sqrt(len(tokens)))  # 16x16\n",
        "    grid = tokens.reshape(grid_size, grid_size)\n",
        "\n",
        "    # Normalize\n",
        "    grid = grid / (grid.max() + 1e-8)\n",
        "\n",
        "    # Upsample to 64x64\n",
        "    grid = tf.image.resize(\n",
        "        grid[..., tf.newaxis],\n",
        "        (image_size, image_size),\n",
        "        method=\"nearest\"\n",
        "    )\n",
        "\n",
        "    # RGB channels\n",
        "    r = grid\n",
        "    g = tf.roll(grid, shift=4, axis=0)\n",
        "    b = tf.roll(grid, shift=8, axis=1)\n",
        "\n",
        "    image = tf.concat([r, g, b], axis=-1)\n",
        "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 3. MAIN GENERATION FUNCTION\n",
        "# =========================================================\n",
        "\n",
        "def generate_pokemon(prompt):\n",
        "    \"\"\"\n",
        "    Text → visual tokens → colored image\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Text → tokens\n",
        "    text_tokens = text_vectorizer([prompt])\n",
        "\n",
        "    # 2. Generate visual tokens\n",
        "    visual_tokens = generate_image_tokens(\n",
        "        transformer_model,\n",
        "        text_tokens, # Removed the extra 256 and 1024 arguments\n",
        "        num_visual_tokens=256 # Explicitly set if not using default\n",
        "    )\n",
        "\n",
        "    # 3. Decode to image\n",
        "    image = decode_to_real_image(visual_tokens)\n",
        "\n",
        "    # 4. Show result\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(image)\n",
        "    plt.title(prompt)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 4. TEST\n",
        "# =========================================================\n",
        "\n",
        "generate_pokemon(\"a pink cute pokemon\")"
      ],
      "metadata": {
        "id": "HaAKCjf6wN4b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}