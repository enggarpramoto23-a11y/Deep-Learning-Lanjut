{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMbU51S71nn+O3QPgIipITR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enggarpramoto23-a11y/Deep-Learning-Lanjut/blob/main/Pak_Anam_Pert11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2su6VVyZI6ba"
      },
      "outputs": [],
      "source": [
        "!pip install datasets diffusers transformers accelerate\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "# 1. Load Dataset\n",
        "print(\"Memuatdataset...\")\n",
        "raw_dataset= load_dataset(\"reach-vb/pokemon-blip-captions\", split=\"train\")\n",
        "# 2. Ambil daftar caption untukproses adaptasi teks\n",
        "all_captions= [item['text'] for item in raw_dataset]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Setup Text Vectorization\n",
        "max_tokens = 5000\n",
        "seq_len = 20\n",
        "\n",
        "text_vectorizer = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_sequence_length=seq_len,\n",
        ")\n",
        "\n",
        "# Proses Adapt (mempelajari kosakata dari dataset)\n",
        "text_vectorizer.adapt(all_captions)\n",
        "\n",
        "vocab = text_vectorizer.get_vocabulary()\n",
        "\n",
        "print(f\"Kamus Teks Berhasil Dibuat. Jumlah kosakata: {len(vocab)}\")\n",
        "print(\"Contoh 10 kata pertama:\", vocab[:10])\n"
      ],
      "metadata": {
        "id": "K7aqU8juKh0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_fn(item):\n",
        "    # Proses Gambar\n",
        "    image = item['image'].convert(\"RGB\").resize((64, 64))\n",
        "    image = np.array(image) / 255.0  # Normalisasi0-1\n",
        "    # Proses Teks\n",
        "    caption = item['text']\n",
        "    return caption, image\n",
        "# Membuatgenerator dataset\n",
        "def gen():\n",
        "    for item in raw_dataset:\n",
        "        yield preprocess_fn(item)\n",
        "\n",
        "# Membuattf.data.Dataset\n",
        "train_ds= tf.data.Dataset.from_generator(\n",
        "    gen,\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(), dtype=tf.string),\n",
        "        tf.TensorSpec(shape=(64, 64, 3), dtype=tf.float32)\n",
        "    )\n",
        ")\n",
        "# Batching dan TransformasiTeks keAngka\n",
        "train_ds= train_ds.map(lambda x, y: (text_vectorizer(x), y))\n",
        "train_ds = train_ds.batch(16).shuffle(100).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "5g1AJnAjKzBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PokemonTrainer(keras.Model):\n",
        "    def __init__(self, transformer, vqvae_encoder):\n",
        "        super().__init__()\n",
        "        self.transformer= transformer\n",
        "        self.vqvae_encoder= vqvae_encoder\n",
        "        self.loss_tracker= keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "    def train_step(self, data):\n",
        "        text_tokens, images = data\n",
        "        # 1. Ubahgambaraslimenjaditoken visual menggunakanencoder\n",
        "        # Kita simulasikandenganoutput dummy sesuaiukuranlatent grid (misal16x16)\n",
        "        visual_tokens= tf.random.uniform((tf.shape(images)[0], 256), minval=0, maxval=1024, dtype=tf.int32)\n",
        "\n",
        "        # 2. Siapkaninput dan target (Autoregressive)\n",
        "        vis_input= visual_tokens[:, :-1]\n",
        "        vis_target= visual_tokens[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Prediksi\n",
        "            preds = self.transformer([text_tokens, vis_input], training=True)\n",
        "            # HitungLoss\n",
        "            loss = keras.losses.sparse_categorical_crossentropy(vis_target, preds, from_logits=True)\n",
        "        grads = tape.gradient(loss, self.transformer.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.transformer.trainable_variables))\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}"
      ],
      "metadata": {
        "id": "PutXjquCLGPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PokemonTrainer(keras.Model):\n",
        "    def __init__(self, transformer, vqvae_encoder):\n",
        "        super().__init__()\n",
        "        self.transformer = transformer\n",
        "        self.vqvae_encoder = vqvae_encoder\n",
        "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        text_tokens, images = data\n",
        "\n",
        "        # Encode image â†’ visual tokens\n",
        "        # Menggunakan output dummy untuk visual_tokens untuk saat ini\n",
        "        visual_tokens = tf.random.uniform((tf.shape(images)[0], 256), minval=0, maxval=1024, dtype=tf.int32)\n",
        "\n",
        "        # Autoregressive shift\n",
        "        vis_input = visual_tokens[:, :-1]\n",
        "        vis_target = visual_tokens[:, 1:]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = self.transformer(\n",
        "                [text_tokens, vis_input],\n",
        "                training=True\n",
        "            )\n",
        "\n",
        "            loss = keras.losses.sparse_categorical_crossentropy(\n",
        "                vis_target, preds, from_logits=True\n",
        "            )\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        grads = tape.gradient(loss, self.transformer.trainable_variables)\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(grads, self.transformer.trainable_variables)\n",
        "        )\n",
        "\n",
        "        self.loss_tracker.update_state(loss)\n",
        "        return {\"loss\": self.loss_tracker.result()}\n",
        "\n",
        "# --- Definisi model yang hilang akan ditambahkan di sini ---\n",
        "# Dummy transformer_model dan vqvae_encoder untuk memungkinkan eksekusi\n",
        "# Anda perlu mengganti ini dengan implementasi model yang sebenarnya\n",
        "# Misalnya:\n",
        "num_visual_tokens = 1024 # Contoh, sesuaikan dengan ukuran vocabulary VQ-VAE Anda\n",
        "embedding_dim = 256 # Contoh, ukuran embedding\n",
        "num_heads = 4 # Contoh, jumlah attention heads\n",
        "ff_dim = 1024 # Contoh, dimensi feed-forward\n",
        "\n",
        "# Dummy VQVAE Encoder (hanya sebagai placeholder)\n",
        "vqvae_encoder = keras.Sequential([\n",
        "    layers.Input(shape=(64, 64, 3)),\n",
        "    layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
        "    layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\"),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(256, activation=\"relu\") # Output ukuran yang sama dengan visual_tokens dummy\n",
        "], name=\"vqvae_encoder_dummy\")\n",
        "\n",
        "# Dummy Transformer (membutuhkan input teks dan visual)\n",
        "text_input = keras.Input(shape=(seq_len,), dtype=\"int32\", name=\"text_input\")\n",
        "visual_input = keras.Input(shape=(255,), dtype=\"int32\", name=\"visual_input\") # 256-1 = 255\n",
        "\n",
        "text_embeddings = layers.Embedding(max_tokens, embedding_dim)(text_input)\n",
        "visual_embeddings = layers.Embedding(num_visual_tokens, embedding_dim)(visual_input)\n",
        "\n",
        "# Concatenate text and visual embeddings\n",
        "x = layers.Concatenate(axis=1)([text_embeddings, visual_embeddings])\n",
        "x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(x, x)\n",
        "x = layers.Dense(ff_dim, activation=\"relu\")(x)\n",
        "\n",
        "# Extract only the visual part of the output sequence to match vis_target length\n",
        "visual_output_sequence = x[:, seq_len:, :]\n",
        "\n",
        "# Remove softmax activation, as sparse_categorical_crossentropy expects logits with from_logits=True\n",
        "outputs = layers.Dense(num_visual_tokens)(visual_output_sequence)\n",
        "\n",
        "transformer_model = keras.Model(inputs=[text_input, visual_input], outputs=outputs, name=\"transformer_model_dummy\")\n",
        "\n",
        "# Inisialisasi dan compile\n",
        "trainer = PokemonTrainer(transformer_model, vqvae_encoder)\n",
        "trainer.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4)\n",
        ")\n",
        "\n",
        "# Jalankan training\n",
        "print(\"Memulai pelatihan...\")\n",
        "trainer.fit(train_ds, epochs=10)  # coba 50 epoch dulu"
      ],
      "metadata": {
        "id": "SBrh8-YSLHDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image_tokens(transformer_model, text_tokens, sequence_length, num_visual_tokens):\n",
        "    generated_visual_tokens = tf.zeros((tf.shape(text_tokens)[0], 0), dtype=tf.int32)\n",
        "    # Start with a dummy token or a start-of-sequence token if defined\n",
        "    # For simplicity, let's start with an empty sequence and let the transformer predict from text\n",
        "\n",
        "    # The transformer expects visual_input length to be seq_len-1 (255) based on dummy definition\n",
        "    # We need to generate 256 tokens. Let's make it more flexible.\n",
        "\n",
        "    # Let's assume an initial visual input for the transformer to start generating.\n",
        "    # A single 'start' token could be used, or we generate the first token based on text, then autoregress.\n",
        "\n",
        "    # Let's adjust the generation loop to match the expected output of 256 visual tokens\n",
        "    for i in tf.range(sequence_length):\n",
        "        # Prepare visual_input for the transformer\n",
        "        # If no tokens generated yet, provide a dummy start token or handle initial prediction\n",
        "        if tf.shape(generated_visual_tokens)[1] == 0:\n",
        "            # For the very first prediction, we might pass a special start token or zeros.\n",
        "            # Given the dummy transformer, let's pass a sequence of zeros for now.\n",
        "            # The `visual_input` in the transformer was defined with shape (255,).\n",
        "            # This implies we generate up to 255 visual tokens after initial setup, or the transformer input\n",
        "            # is expected to be a partial sequence. Let's simplify this for the placeholder.\n",
        "\n",
        "            # For simplicity in this placeholder, we will simulate autoregressive generation\n",
        "            # by feeding the currently generated sequence (padded if too short).\n",
        "\n",
        "            # Create a visual_input tensor that is 'sequence_length - 1' long (255)\n",
        "            # and pad with zeros if the generated_visual_tokens is shorter.\n",
        "            current_visual_input_len = tf.shape(generated_visual_tokens)[1]\n",
        "            if current_visual_input_len < sequence_length - 1: # 255\n",
        "                padding = tf.zeros((tf.shape(text_tokens)[0], (sequence_length - 1) - current_visual_input_len), dtype=tf.int32)\n",
        "                transformer_visual_input = tf.concat([generated_visual_tokens, padding], axis=1)\n",
        "            else:\n",
        "                transformer_visual_input = generated_visual_tokens[:, -(sequence_length - 1):]\n",
        "\n",
        "        else:\n",
        "            current_visual_input_len = tf.shape(generated_visual_tokens)[1]\n",
        "            if current_visual_input_len < sequence_length - 1:\n",
        "                padding = tf.zeros((tf.shape(text_tokens)[0], (sequence_length - 1) - current_visual_input_len), dtype=tf.int32)\n",
        "                transformer_visual_input = tf.concat([generated_visual_tokens, padding], axis=1)\n",
        "            else:\n",
        "                transformer_visual_input = generated_visual_tokens[:, -(sequence_length - 1):]\n",
        "\n",
        "        # Get predictions for the next token\n",
        "        preds = transformer_model([text_tokens, transformer_visual_input], training=False)\n",
        "\n",
        "        # The output of transformer_model is `outputs = layers.Dense(num_visual_tokens)(visual_output_sequence)`\n",
        "        # `visual_output_sequence` length will be `sequence_length - 1` (255) in dummy transformer\n",
        "        # We need the prediction for the *next* token. So, we take the last prediction.\n",
        "        next_token_logits = preds[:, -1, :]\n",
        "\n",
        "        # Sample the next token (e.g., using argmax for deterministic generation)\n",
        "        next_token = tf.cast(tf.argmax(next_token_logits, axis=-1), dtype=tf.int32)\n",
        "        next_token = tf.expand_dims(next_token, axis=1)\n",
        "\n",
        "        # Append the new token to the generated sequence\n",
        "        generated_visual_tokens = tf.concat([generated_visual_tokens, next_token], axis=1)\n",
        "\n",
        "    # Ensure the final output has exactly `sequence_length` tokens\n",
        "    # In the trainer, visual_tokens had a shape of 256. So we generate 256.\n",
        "    # The loop runs for `sequence_length` times, let's assume sequence_length=256 for visual tokens.\n",
        "    return generated_visual_tokens[:, :sequence_length]\n",
        "\n",
        "def decode_to_real_image(visual_tokens):\n",
        "    # This is a placeholder function since the VQ-VAE decoder is not implemented.\n",
        "    # In a real scenario, this would use the VQ-VAE decoder to convert visual tokens\n",
        "    # back into an image.\n",
        "\n",
        "    # For now, return a black image or a random image of the expected size (64x64x3)\n",
        "    # The visual_tokens might not be used in this dummy implementation.\n",
        "    dummy_image = tf.zeros((1, 64, 64, 3), dtype=tf.float32)\n",
        "    return dummy_image[0]\n",
        "\n",
        "\n",
        "def generate_pokemon(prompt):\n",
        "    # 1. Ubahtekskeangka\n",
        "    tokenized_text= text_vectorizer([prompt])\n",
        "    # 2. Generate token visual (Autoregressive)\n",
        "    # Gunakanfungsigenerate_image_tokensyang kitabuatsebelumnya\n",
        "    # Note: generate_image_tokens and decode_to_real_image are not yet defined.\n",
        "    # We'll need to define these functions or integrate a proper VQ-VAE model.\n",
        "    # For now, let's assume they exist as placeholders for the indentation fix.\n",
        "\n",
        "    # The dummy visual_tokens in trainer were 256. Let's use that as the target length.\n",
        "    vis_token_sequence_len = 256\n",
        "    gen_vis_tokens= generate_image_tokens(transformer_model, tokenized_text, vis_token_sequence_len, num_visual_tokens)\n",
        "    # 3. Decode jadiGambar menggunakanPre-trained VAE\n",
        "    # Gunakanfungsidecode_to_real_imageyang memanggilAutoencoderKL\n",
        "    final_image= decode_to_real_image(gen_vis_tokens)\n",
        "    plt.imshow(final_image)\n",
        "    plt.title(prompt)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# TEST\n",
        "generate_pokemon(\"a pink cute pokemon\")"
      ],
      "metadata": {
        "id": "HaAKCjf6wN4b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}